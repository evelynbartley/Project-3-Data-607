---
title: "DATA 607 Project 3"
author: "Evelyn Bartley, Dhanya Nair, Alexander Simon, AJ Strauman-Scott, Peter Thompson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load database interface package
if (!require(RPostgres)) print("RPostgres package not installed")
library(RPostgres)
library(RColorBrewer)
library(tidyverse)
library(wordcloud)
library(kableExtra)
library(hexbin)
library(DBI)

# Establish database connection
sql_db <- dbConnect(
  RPostgres::Postgres(),
  dbname = "project3",
  host = '35.199.10.112',
  port = 5432,
  user = 'group',
  password = '1234'
)
```

\<\<\< Can we create a table of contents? \>\>\>

# Introduction

We selected the Data Science Skills option for Project 3. Here, we describe the identification, collection, database storage, and analyses of data to answer the question, "Which are the most valued data science skills?"

We divided this question into two partsâ€“hard skills and soft skills-and analyzed each separately. For the hard skills analysis, we assessed the value of a skill by its frequency of occurrence (ie, a skill that is mentioned many times in a dataset would be a highly valued skill) and by its monetary value. For the soft skills analysis, we only evaluated the value of a skill by the frequency of its occurrence.

<br>

# [Hard skills analysis]{.underline}

## Data overview

The hard skills analysis is based on the 2023 [Stack Overflow Annual Developer Survey](https://insights.stackoverflow.com/survey/), which includes survey responses from more than 90,000 software developers from 185 countries. The survey was fielded from May 8, 2023 to May 19, 2023.

\<\<\<How was 185 determined? The Countries section below shows 91 unique countries. \>\>\>

For analysis, we only considered respondents who indicated in the survey that they consented to share their information and answered all the required questions. Approximately 2,000 responses were excluded based on these criteria, leaving 89,184 for analysis.

We downloaded the original CSV data file from Stack Overflow and saved it in our team GitHub repository.

```{r load-data}
#Load the original .csv file
survey_2023 <- read.csv("https://media.githubusercontent.com/media/alexandersimon1/Data607/main/Project3/survey_results_public.csv", header = TRUE)
```

### Data transformations

We filter the survey for only the observations of interest - those who identified as "Data scientists or Machine-Learning Specialist" and "a developer by profession", so that all survey results reflect our population of interest - professional, working data scientists.

```{r filter-data}
# Filter for the observations of interest
datascientists_2023 <- survey_2023 %>% 
  filter(DevType == "Data scientist or machine learning specialist",
         MainBranch == "I am a developer by profession")
```

<br>

### Database Design

Firstly we created the ER diagram. we used this to create our logical model.
Once the logical model was created, we used it to create a normalized relational tables in postgreSQL.

The normalization was done considering the following factors:
1. the variable values repeated for many observations  were normalized into separate tables by creating IDs.
2. we could add further group attributes  to the dimensional tables once we normalized the main table.

Generated ID tables in PostgreSQL for all the possible answers from the survey for each topic we want to examine.

<br>

## Data Tidying

The survey data will be tidied in R, pushed to pre-made relational tables in PostgreSQL, and then analyzed through R.

### Tidy the data

**NOTE to authors** - because there is already a table with the USD value of an observation's compensation already converted, I'm using that column for our 'compensation' column

```{r tidy-ds_2023, warning=FALSE}
# Match df to 'employee' table format in DB
employees_df <- datascientists_2023 %>% 
  mutate(response_id = ResponseId,
         country = Country,
         job_title = as.factor(DevType),
         industry = as.factor(Industry), 
         education = as.factor(EdLevel),
         office_location = as.factor(RemoteWork),
         compensation = as.numeric(ConvertedCompYearly),
         years = as.numeric(YearsCodePro),
        language = LanguageHaveWorkedWith,
        platform = PlatformHaveWorkedWith,
        database = DatabaseHaveWorkedWith,
        collab_tools = NEWCollabToolsHaveWorkedWith,
        ai_tools = str_c(AISearchHaveWorkedWith, AIDevHaveWorkedWith, sep = ";")
  ) %>% 
  select(response_id, 
         country,
         industry,
         education,
         office_location,
         compensation,
         years,
         language,
         platform,
         database,
         collab_tools,
         ai_tools)
```

<br>

### Normalize multiple answers

Our variables of interest were all in response to select-all-that-apply survey questions. In order to record each of the answers, each of the 5 multi-choice answer columns must be split so that there is one answer per column in a relational table in the database.

We pivot our survey results longer so that every checkbox has an observation. But if we keep them in the same table, we'll double our data.

We convert multiple choice questions to factors for ease-of-use later.

```{r create-factors, echo=FALSE}
# Columns of interest to factors
employees_df <- employees_df %>% 
  mutate(language_id = as.factor(language),
         platform_id = as.factor(platform),
         db_id = as.factor(database),
         tool_id = as.factor(ifelse(collab_tools == "Unknown", NA, collab_tools)),
         ai_id = as.factor(ifelse(ai_tools == "Unknown", NA, ai_tools))
  )
```

We define functions for algorithms we will run repeatedly. I tried to write a function that did all the steps to normalize each survey question, but it was too nuanced.

```{r define-functions, echo=FALSE}
# Define split_and_unnest function
split_and_unnest <- function(data, columns, na_replace = "") {
  split_data <- lapply(columns, function(col) str_split(data[[col]], ";"))
  names(split_data) <- columns
  
  split_data <- lapply(split_data, function(x) replace(x, is.na(x), na_replace))
  
  data <- data %>%
    mutate(across(all_of(columns), ~split_data[[cur_column()]])) %>%
    unnest(cols = all_of(columns))
  
  return(data)
}

# define convert_to_factors function
convert_to_factors <- function(data, columns) {
  data <- data %>%
    mutate(
      across(all_of(columns), as.factor)
    )
  return(data)
}
```

Write the employees table to the database.

```{r write-employees-table, echo=FALSE}
# Create employees table
employees_sql <- employees_df %>% 
  select(response_id, country, industry, education, office_location, compensation, years)

# Write to DB
# Line removed to prevent data duplication
```

Normalize each survey question we are interested in to its own table.

```{r write-employee-languages, echo=FALSE}
# Create df to write to 'employee-languages' table
languages_df <- employees_df %>%
  select(response_id, language_id) %>% 
  split_and_unnest('language_id', na_replace = "Unknown") 
  
# Query id table
language_id <- dbGetQuery(sql_db, "SELECT * FROM id_languages") %>%
  convert_to_factors(c("language_id", "language"))

# Swap values for id numbers
languages_df$language_id <- language_id$language_id[match(languages_df$language_id, language_id$language)]  %>% as.numeric()

# Write to Db table
# Line removed to prevent data duplication
```

Automated as much as I could in the function definitions above. Repeat these steps for the other tables

```{r write-employee-platforms, echo=FALSE}
# Create df to write to 'employee-platforms' table
platforms_df <- employees_df %>%
  select(response_id, platform_id) %>% 
  split_and_unnest('platform_id', na_replace = "Unknown") 

# Query id table
platform_id <- dbGetQuery(sql_db, "SELECT * FROM id_platforms") %>%
  convert_to_factors(c("platform_id", "platform"))

# Swap values for id numbers
platforms_df$platform_id <- platform_id$platform_id[match(platforms_df$platform_id, platform_id$platform)]  %>% as.numeric()

# Write to Db table
# Line removed to prevent data duplication
```

```{r write-employee-databases, echo=FALSE}
# Create df to write to 'employee-databases' table
db_df <- employees_df %>%
  select(response_id, db_id) %>% 
  split_and_unnest('db_id', na_replace = "Unknown") 

# Query id table
db_id <- dbGetQuery(sql_db, "SELECT * FROM id_databases") %>%
  convert_to_factors(c("db_id", "database"))

# Swap values for id numbers
db_df$db_id <- db_id$db_id[match(db_df$db_id, db_id$database)]  %>% as.numeric()

# Write to DB table
# Line removed to prevent data duplication
```

```{r write-employee-collab-tools, echo=FALSE}
# Create df to write to 'employee_collab_tools' table
tools_df <- employees_df %>%
  select(response_id, tool_id) %>% 
  split_and_unnest('tool_id', na_replace = "Unknown") 

# Query id table
tool_id <- dbGetQuery(sql_db, "SELECT * FROM id_collab_tools") %>%
  convert_to_factors(c("tool_id", "tool"))

# Swap values for id numbers
tools_df$tool_id <- tool_id$tool_id[match(tools_df$tool_id, tool_id$tool)] %>% as.numeric()

# Write to DB table
# Line removed to prevent data duplication
```

```{r write-employee-ai-tools, echo=FALSE}
# Create df to write to 'employee_ai_tools' table
ai_df <- employees_df %>%
  select(response_id, ai_id) %>% 
  split_and_unnest('ai_id', na_replace = "Unknown") 

# Query id table
ai_id <- dbGetQuery(sql_db, "SELECT * FROM id_ai_tools") %>%
  convert_to_factors(c("ai_id", "ai_tool"))

# Swap values for id numbers
ai_df$ai_id <- ai_df$ai_id <- ai_id$ai_id[match(ai_df$ai_id, ai_id$ai_tool)] %>% as.numeric()

# Write to DB table
# Line removed to prevent data duplication
```

```{r employees_df-col-of-interest, include=FALSE}
employees_df <- employees_df %>% 
  select(response_id, country, industry, education, office_location, compensation, years, language, platform, database, collab_tools, ai_tools)

kable(employees_df)
```


All our relational tables are created and have their data.

Our SQL relational database is now clean and tidy! I have removed the functions that write to the SQL database so our tables don't get overwritten. They were all variations of `dbWriteTable(sql_db, "table_name", table_df, append = TRUE)`

This is diagram of our relational database:

![Entity-relationship diagram](https://github.com/evelynbartley/Project-3-Data-607/blob/main/images/ER_diagram.png?raw=true)

## Analysis

Do analysis here!

```{r list-tables}
dbListTables(sql_db)
```

## Exploratory data analysis

### language skills
Python is  the most popular language skill for data scientists.
followed by SQL, Bash/Shell , Javascript, HTML/CSS,R
```{r}
# count the occurrence of languages in the survey , group the occurrences more than less than 100 times into "others"  and finally arrange in the descending order and plot in a column chart.

language_skills <- languages_df 
  language_skills$language_id <- language_id$language[match(language_skills$language_id,language_id$language_id)] 


language_skills %>% 
  count(language_id)%>% 
  mutate(language_id = if_else(n > 100, language_id, "others")) %>% 
  ggplot(aes(x = fct_reorder(language_id,n), y = n)) + 
    geom_col(fill = "lightblue") + 
  coord_flip() +
    labs(x = "languages", y = "most valuable data science - language skills", caption = "https://insights.stackoverflow.com/survey/")


```


### Database skills
PostgreSQL is  the most popular language skill for data scientists.
followed by MySQL, SQLite and Microsoft SQL server

```{r}
#creating a new database skills table by joining the employees_database table and the database_Id table to get the description of the IDs.
db_skills <- db_df 
db_skills$db_id <- db_id$database[match(db_skills$db_id,db_id$db_id)] 

#Filtering the rows which have NA as the database
db_skills <- subset(db_skills,db_id != "NA")

  
# count the occurrence of databases in the survey , group the occurrences less than 100 times into "others"  and finally arrange in the descending order and plot in a column chart.
  db_skills %>% 
  count(db_id)%>% 
  mutate(db_id = if_else(n > 100, db_id, "others")) %>% 
  ggplot(aes(x = fct_reorder(db_id,n), y = n)) + 
    geom_col(fill = "lightblue") + 
  coord_flip() +
    labs(x = "Databases", y = "most valuable data science - Database skills", caption = "https://insights.stackoverflow.com/survey/")

```

### Tool skills
Visual Studio Code is the most popular language skill for data scientists.
followed by Jupyter Notebook/lab , PyCharm ,IPython,Vim,Notepad++, RStudio



```{r}

#creating a new database skills table by joining the employees_database table and the database_Id table to get the description of the IDs.
tool_skills <- tools_df 
tool_skills$tool_id <- tool_id$tool[match(tool_skills$tool_id,tool_id$tool_id)] 

#Filtering the rows which have NA as the database
#db_skills <- subset(_skills,db_id != "NA")

  
# count the occurrence of databases in the survey , group the occurrences less than 100 times into "others"  and finally arrange in the descending order and plot in a column chart.
  tool_skills %>% 
  count(tool_id)%>% 
  mutate(tool_id = if_else(n > 100, tool_id, "others")) %>% 
  ggplot(aes(x = fct_reorder(tool_id,n), y = n)) + 
    geom_col(fill = "lightblue") + 
  coord_flip() +
    labs(x = "Databases", y = "most valuable data science - Database skills", caption = "https://insights.stackoverflow.com/survey/")

```




### Countries

Data scientists in the dataset were from 91 countries.

```{r count-employee-countries}
survey_countries <- employees_df %>%
  group_by(country) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  rename(region = country)
```


```{r get-map-data}
library(ggplot2)
library(maps)
library(ggmap)
mapdata <- map_data("world")
mapdata <- mapdata %>%
  mutate(
    region = str_replace(region, "Hong Kong", "Hong Kong (S.A.R.)"),
    region = str_replace(region, "Iran", "Iran, Islamic Republic of"),
    region = str_replace(region, "North Macedonia", "The former Yugoslav Republic of Macedonia"),    
    region = str_replace(region, "Russia", "Russian Federation"),
    region = str_replace(region, "UK", "United Kingdom of Great Britain and Northern Ireland"),
    region = str_replace(region, "USA", "United States of America"),    
    region = str_replace(region, "Vietnam", "Viet Nam")
  )
```

```{r combine-map-data}
map_surveys <- left_join(mapdata, survey_countries, by = "region")
```

The map shows that although survey responses were received from 91 countries, most responsess were from the US. Countries without any survey responses are shown in gray.

```{r create-map, fig.width=6, warning = FALSE}
ggplot(map_surveys, aes(x = long, y = lat, group=group)) +
  coord_fixed(1.3) +
  geom_polygon(aes(fill = n)) +
  scale_fill_distiller(palette = "RdBu", direction = -1, name = "n") +
  theme_void() + 
  theme(legend.position = c(0.1, 0.35), legend.title = element_text(hjust = 0.15))
```

<br>

### Education level

```{r count-education-levels}
survey_education <- employees_df %>%
  group_by(education) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  mutate(
    education = str_replace(education, "Professional.*", "Advanced"), 
    education = str_replace(education, "Master.*", "Master's"),     
    education = str_replace(education, "Bachelor.*", "Bachelor"),    
    education = str_replace(education, "Associate.*", "Associate"),    
    education = str_replace(education, "Some college.*", "Some college"),
    education = str_replace(education, "Secondary.*", "High school"),
    education = str_replace(education, "Primary.*", "Primary")    
  )
```

Most data scientists had a master's degree.

```{r most-common-degrees-barplot}
ggplot(survey_education, aes(x = reorder(education, n), y = n)) +
  geom_col() + 
  coord_flip() +
  xlab("Highest degree") + theme(axis.title = element_text(face = "bold")) +
  scale_y_continuous(breaks = seq(0, 700, by = 100)) 
```

<br>

### Industry type

```{r count-industry-types}
survey_industry <- employees_df %>%
  drop_na(industry) %>%
  group_by(industry) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  mutate(
    industry = str_replace(industry, "Information.*", "IT"),
    industry = str_replace(industry, "Financial.*", "Finance"),
    industry = str_replace(industry, "Manufacturing.*", "Manufacturing"),
    industry = str_replace(industry, "Retail.*", "Retail"),
    industry = str_replace(industry, "Advertising.*", "Advertising"),
    industry = str_replace(industry, "Higher.*", "Education"),
    industry = str_replace(industry, "Legal.*", "Legal")
  )
```

Data scientists were mostly employed in IT. The next most common fields (other than "Other") were finance, manufacturing, and healthcare.

```{r most-common-industries-barplot}
ggplot(survey_industry, aes(x = reorder(industry, n), y = n)) +
  geom_bar(stat = 'identity') + 
  coord_flip()  +
  xlab("Industry") + theme(axis.title = element_text(face = "bold")) +
  scale_y_continuous(breaks = seq(0, 300, by = 50)) 
```

<br>

### Annual compensation

The median annual compensation was 79,581 USD.

```{r compensation-statistics}
survey_compensation <- employees_df %>%
  select(compensation) %>%
  drop_na()

survey_compensation %>%
  summarise(
    min = min(compensation),
    max = max(compensation),
    mean = mean(compensation),
    SD = sd(compensation),
    median = median(compensation),
    IQR = IQR(compensation)
  ) %>%
  kbl() %>% 
  kable_material()
```

The distribution is unimodal and skewed to the right by some very large values. To improve the visualization, values \>1,000,000 USD were aggregated into an outlier bin in the histogram.

```{r compensation-histogram}
survey_compensation <- survey_compensation %>%
  mutate(
    compensation_adjusted = if_else(compensation > 1000000, 1000000, compensation)
  )

ggplot(survey_compensation, aes(x = compensation_adjusted)) +
  geom_histogram(binwidth = 5000) +
  xlab("Compensation (USD)") + theme(axis.title = element_text(face = "bold")) +  
  scale_x_continuous(breaks = c(0, 250000, 500000, 750000, 1000000), 
                     labels = c("0", "250000", "500000", "750000", ">1000000"))
```

<br>

### Annual compensation vs years of experience

```{r compensation-experience}
survey_comp_experience <- employees_df %>%
  select(compensation, years) %>%
  drop_na()
```

Due to the large number of overlapping data points, a hexagonal heatmap was used to show the relationshiop between years of experience and annual compensation. In the heatmap, data points are binned into hexagons, which are filled with a color gradient corresponding to the number of data points in each hexagon. The red hexagons shows that most data scientists who completed the survey had \<5 years of experience and earned \<100,000 USD.

Surprisingly, greater experience was not associated with higher compensation. Some data scientists with \<15 years of experience earned large salaries, and those with \>20 years of experience earned similar amounts as those who just entered the field. This suggests that other factors besides years of experience influence the compensation of data scientists.

```{r comp-experience-scatterplot, warning = FALSE}
ggplot(survey_comp_experience, aes(x = years, y = compensation)) +
  stat_binhex(bins = 50) +
  scale_fill_gradient(low = "lightblue", high = "red") +
  ylim(0, 1000000) + 
  xlab("Years of experience") + ylab("Compensation (USD)") + 
  theme(axis.title = element_text(face = "bold"))
```

<br>

### Industry vs annual compensation

Omit rows with missing information, shorten industry names, then group the data by industry.

```{r group-by-industry}
industry_comp <- employees_df %>%
  select(industry, compensation) %>%
  drop_na() %>%
  mutate(
    industry = str_replace(industry, "Information.*", "IT"),
    industry = str_replace(industry, "Financial.*", "Finance"),
    industry = str_replace(industry, "Manufacturing.*", "Manufacturing"),
    industry = str_replace(industry, "Retail.*", "Retail"),
    industry = str_replace(industry, "Advertising.*", "Advertising"),
    industry = str_replace(industry, "Higher.*", "Education"),
    industry = str_replace(industry, "Legal.*", "Legal")
  ) %>%  
  group_by(industry)  
```

The fields with the highest median compensation for data scientists were insurance, finance, and legal, which were all more than 100,000 USD. Surprisingly, IT, which has the largest proportion of data scientists in the survey, is associated with the third lowest median compensation (\~64,000 USD).

```{r median-compensation-by-industry}
industry_comp %>%
  summarise(
    median_compensation = median(compensation)
  ) %>%
  arrange(desc(median_compensation)) %>%
  kbl() %>%
  kable_material() %>%
  scroll_box(width = "75%", height = "500px")
```

<br>

The distribution of median annual compensation by industry is shown below. Note that outliers are not plotted. Some industries, such as oil & gas and healthcare had a relatively large spread (IQR) in compensation, while others, such as insurance and legal, had smaller spreads.

```{r compensation-by-industry-boxplots, warning = FALSE, fig.height=4, fig.width=8}
industry_comp %>%
  group_by(industry) %>%
  ggplot(aes(x = compensation, group = industry, color = industry)) +
  geom_boxplot(outlier.shape = NA) +
    labs(x = "Annual compensation (USD)") +
    theme(axis.title = element_text(face = "bold"), 
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank(),
          strip.text.x = element_text(size = 8)) +  
    scale_x_continuous(breaks = c(0, 50000, 100000), 
                       limits = c(0, 100000),
                       labels = scales::scientific) +
  facet_wrap(~ industry) +
    theme(legend.position = "none") +
    theme(panel.spacing.x = unit(1, "lines"))
```

<br>

### Language vs annual compensation

Convert the language variable to a long format by separating the semicolon-delimited string into individual languages. Then group the data by language.

```{r separate-languages}
language_comp <- employees_df %>%
  select(language, compensation) %>%
  drop_na() %>%
  separate_longer_delim(language, delim = ";") %>%
  group_by(language)  
```

COBOL was associated with the highest median compensation, which was surprising because it is a very old language. It is possible that knowledge and proficiency with it is rare nowadays and therefore of high value to companies with legacy systems.

The median annual compensation associated with the languages commonly used in data science (eg, Python, R, SQL) was lower than expectedâ€”around 80,000 USD.

```{r median-compensation-by-language}
language_comp %>%
  summarise(
    median_compensation = median(compensation)
  ) %>%
  arrange(desc(median_compensation)) %>%
  kbl() %>%
  kable_material() %>%
  scroll_box(width = "75%", height = "500px")
```

<br>

The distribution of annual compensation by language is shown below. Note that outliers are not plotted.

```{r compensation-by-language-boxplots, warning = FALSE, fig.width=10, fig.height=8}
language_comp %>%
  group_by(language) %>%
  ggplot(aes(x = compensation, group = language, color = language)) +
  geom_boxplot(outlier.shape = NA) +
    labs(x = "Annual compensation (USD)") +
    theme(axis.title = element_text(face = "bold"), 
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank(),
          strip.text.x = element_text(size = 8)) +  
    scale_x_continuous(breaks = c(0, 100000, 200000), limits = c(0, 250000)) +
  facet_wrap(~ language, ncol = 4) +
    theme(legend.position = "none") +
    theme(panel.spacing.x = unit(1, "lines"))  
```

<br>

# [Soft skills analysis]{.underline}

## Data

Soft skills data were collected from two different sources.

1.  Informational websites that ranked the most important soft skills for data scientists. A total of 65 websites were identified from the following Google searches (URLs for all sites are in the data file):

    -   "soft skills" "data science"

    -   companies that hire data scientists "soft skills"

    -   data scientist recruit "soft skills"

2.  Kaggle [dataset](https://www.kaggle.com/datasets/asaniczka/data-science-job-postings-and-skills) of 12,217 data science-related job postings on LinkedIn during 2024.

All original data were CSV files and saved to our team GitHub repository.

```{r read-raw-data1}
soft_skills_raw <- read_csv('https://media.githubusercontent.com/media/evelynbartley/Project-3-Data-607/main/data/data_science_soft_skills.csv', show_col_types = FALSE)
```

```{r read-raw-data2}
job_postings_raw <- read_csv('https://media.githubusercontent.com/media/evelynbartley/Project-3-Data-607/main/data/job_postings_all.csv', show_col_types = FALSE)
```

<br>

## Data transformations

### Data from soft skills websites

```{r select-columns1}
soft_skills <- soft_skills_raw %>%
  select(Company, Author_name_last, Author_name_first, Field, Audience, Skill, Rank, Date_publish)
```

No other transformations were necessary with this dataframe because long data formatting and tidy data principles were applied at the time of data collection.

The dataframe looks like this:

```{r glimpse-df1}
glimpse(soft_skills)
```

### Data from job postings

Select relevant columns and convert case

```{r select-columns2, include=FALSE}
job_postings <- job_postings_raw %>%
  select(job_title, job_level, job_skills) %>%
  mutate(
    job_title = toupper(job_title),
    job_level = toupper(job_level),
    job_skills = toupper(job_skills)
  )  
```

The data were limited to jobs with "Data Scientist" in the job title. A total of 809 job postings met this criterion.

```{r filter-ds-jobs}
job_postings <- job_postings %>%
  filter(grepl("DATA SCIENTIST", job_title))

nrow(job_postings)
```

The dataframe looks like this:

```{r glimpse-df2}
glimpse(job_postings)
```

<br>

## Analyses

### Soft skills websites

#### Finding #1: Communication is the most common (valued) soft skill in websites about important soft skills for data scientists

```{r top-skills-websites}
top_skills_overall <- soft_skills %>%
  group_by(Skill) %>%
  tally() %>%
  arrange(desc(n))
```

<br>

We show this with 2 different visualizations. The first is a word cloud, which scales the font size of words to their frequency in a dataset. Of note, word clouds work best with single words, so skills described by more than one word were reworded with a synonym or hyphenated.

Communication is the biggest word in the cloud, which means that it is the most common skill in the dataset. On the other hand, skills with small font sizes, such as intuition and empathy, were not very common.

```{r top-skills-websites-wordcloud, fig.height=6}
set.seed(1234)
wordcloud(words = top_skills_overall$Skill, 
          freq = top_skills_overall$n, min.freq = 3,
          max.words = 50, random.order = FALSE, rot.per = 0,
          colors = brewer.pal(5, "Dark2"))
```

The word cloud is visually appealing but isn't quantitative, so we also show the frequency of the soft skills with a barplot. Only skills that occurred more than once in the dataset are shown.

```{r top-skills-websites-barplot, fig.height=6}
ggplot(filter(top_skills_overall, n > 2), aes(x = reorder(Skill, n), y = n)) +
  geom_bar(stat = 'identity') + 
  coord_flip() +  
  xlab("Soft skill") + ylab("Count") + theme(axis.title = element_text(face = "bold")) +
  scale_y_continuous(breaks = seq(0, 60, by = 10))
```

<br>

#### Finding #2: Communication has consistently been the most common (valued) soft skill over time

Group data by year, drop rows without year info, and filter out skills that only occur once

```{r top-skills-by-year}
top_skills_year <- soft_skills %>%
  group_by(year = lubridate::year(Date_publish), Skill) %>%
  tally() %>%
  drop_na(year) %>%
  filter(n > 2) %>%
  arrange(desc(n))
```

The stacked barplot shows that communication has been the most frequent soft skill since 2021. In addition, the number of skills of interest has increased each year since 2021, which may indicate that the data science field is becoming increasingly competitive.

```{r top-skills-by-year-barplot}
ggplot(top_skills_year, aes(x = factor(year), y = n, fill = Skill)) +
  geom_bar(stat = 'identity', position = 'fill') +
  xlab("Year") + ylab("Proportion") + theme(axis.title = element_text(face = "bold")) +
  labs(caption = "Data as of March 2024")
```

<br>

#### Finding #3: The most common (valued) soft skills vary by target audience

Group data by target audience and skill, then calculate frequencies. To prevent the barplot from being too crowded, only skills that occurred more than 3 times were included in this analysis.

```{r top-skills-by-audience, message = FALSE}
top_skills_audience <- soft_skills %>%
  group_by(Audience, Skill) %>%
  summarise(n = n()) %>%
  filter(n > 3) %>%  
  mutate(freq = n / sum(n)) %>%
  arrange(desc(freq))
```

The stacked barplot shows that websites targeted to students name the most soft skills whereas websites targeted to job seekers name the fewest. Websites targeted to employees and companies (generally human resource departments) are in between. These results suggest that students are being encouraged to cultivate many skills, whereas graduates (ie, job seekers or employees) are being encouraged to hone a few specific skills.

The difference in skills between employees and companies makes intuitive sense. Employees, who are most likely working on problems and interacting with colleagues, need to have problem-solving and analytical skills. On the other hand, companies that are seeking growth and profit would benefit from job candidates who are curious and have entrepreneurial skills.

But the most common skill across all audiences is communication. This also makes sense because it is a general skill and not much can be achieved without it.

```{r top-skills-by-audience-barplot}
order <- c("General", "Students", "Job seekers", "Employees", "Companies")
ggplot(top_skills_audience, aes(x = Audience, y = freq, fill = Skill)) +
  geom_col(position = "fill") +
  scale_x_discrete(limits = order) +
  xlab("Target audience") + ylab("Proportion") + 
  theme(axis.title = element_text(face = "bold"))
```

<br>

### Data science job postings

We first need to define the soft skills of interest. This was done by using the soft skills identified by the web search above and adding some common synonyms.

```{r soft-skills-regex}
soft_skills_terms <- c("ACCOUNTABILITY", "ADAPTABILITY|FLEXIBILITY|RESOURCEFUL", 
                       "ANALYTICAL|CRITICAL|LOGIC", 
                       "ATTENTION TO DETAIL", 
                       "ACUMEN|BUSINESS INTELLIGENCE|ENTREPRENEUR",
                       "COLLABORATION|TEAM", "COMMUNICATION|PRESENTATION|STORY", 
                       "CREATIVITY|INNOVATION", 
                       "CURIOSITY",
                       "EMOTIONAL INTELLIGENCE|EMPATHY", 
                       "INDEPENDENCE|INITIATIVE|MOTIVAT", "INTEGRITY|CREDIBLE|TRUSTWORTHY", 
                       "INTERPERSONAL", "INTUITION",
                       "JUDGMENT",
                       "LEADERSHIP", "LISTEN",
                       "ORGANIZATION|PLANNING", "OWNERSHIP",
                       "PASSION", "PERSUASION", "PERSEVERANCE|PERSISTANCE",
                       "PROBLEM", "PROFESSIONALISM", 
                       "RELIABILITY", 
                       "MULTITASKING|TIME MANAGEMENT|PRIORITIZATION")
```

Now we can count the number of postings with these skills.

```{r filter-rows-postings}
soft_skills_freq <- as_tibble(soft_skills_terms)
soft_skills_freq <- soft_skills_freq %>%
  rename(Skill = value) %>%
  rowwise() %>%
  mutate(
    n = sum(grepl(Skill, job_postings$job_skills))
  ) %>%
  mutate(
    Skill = str_replace(Skill, "([A-Z]*)[ \\|].*", "\\1"),
    Skill = str_to_title(Skill),
    Skill = str_replace(Skill, "Acumen", "Entrepreneurial"),
    Skill = str_replace(Skill, "Emotional Intelligence", "EQ"),
    Skill = str_replace(Skill, "Problem", "Problem-solving")    
  ) %>%  
  arrange(desc(n))
```

<br>

#### Finding #1: Communication is the most common (valued) soft skill in data scientist job postings

As before, we show this with a word cloud and a barplot.

```{r job-postings-skills-wordcloud, fig.height=6, fig.width=6}
set.seed(1234)
wordcloud(words = soft_skills_freq$Skill, 
          freq = soft_skills_freq$n, min.freq = 3,
          max.words = 50, random.order = FALSE, rot.per = 0,
          colors = brewer.pal(5, "Dark2"))
```

```{r top-skills-postings-barplot, fig.height=6}
ggplot(filter(soft_skills_freq, n > 2), aes(x = reorder(Skill, n), y = n)) +
  geom_bar(stat = 'identity') + 
  coord_flip() +  
  xlab("Soft skill") + ylab("Count") + theme(axis.title = element_text(face = "bold")) +
  scale_y_continuous(breaks = seq(0, 400, by = 50))
```

Overall, these results look similar to the website analysis. In both datasets, communication and collaboration were the #1 and #2 most common soft skills. Is there a correlation between the skill rankings in the two datasets? Let's find out!

<br>

#### Finding #2: There is a strong correlation between the rankings of the most common (valued) soft skills for data scientists in informational websites and job postings

To evaluate the correlation, we first combined the rankings from the two dataframes.

```{r compare-rankings}
# Rankings from job postings
soft_skills_freq <- soft_skills_freq %>%
  mutate(
    rank_job_postings = which(soft_skills_freq$Skill == Skill)
  )
# Rankings from soft skills websites
top_skills_overall <- top_skills_overall %>%
  mutate(
    rank_websites = which(top_skills_overall$Skill == Skill)
  )
# Combine rankings
rankings <- full_join(soft_skills_freq, top_skills_overall, by = "Skill")
rankings <- rankings %>%
  select(Skill, rank_job_postings, rank_websites) %>%
  drop_na()
```

Then we plotted the rankings with a scatterplot. The correlation coefficient of the line of best fit for the data was 0.79, which indicates a relatively strong association between the two rankings.

```{r scatterplot-rankings, message = FALSE}
coeff <- round(cor(rankings$rank_job_postings, rankings$rank_websites, 
                   method = c("pearson")), 2)
ggplot(rankings, aes(x = rank_job_postings, y = rank_websites)) +
  geom_point() +
  geom_smooth(method=lm, se=FALSE) +
  xlab("Ranking in job postings") + ylab("Ranking on informational websites") +
    theme(axis.title = element_text(face = "bold")) +
  geom_text(x = 20, y = 32,
            label = paste0('r = ', coeff),
            color = 'blue')  
```

The top 5 skills are:

```{r top5}
head(rankings, 5)
```

In summary, we found similar results from two different data sources, informational websites and job postings. This dual-source approach strengthens the conclusion that the most valued soft skills for data scientists are communication and collaboration, followed by problem-solving, analytical thinking, and an entrepreneurial mindset.

<br>

# [Overall conclusions]{.underline}

<br>

## Bibliography

## 
