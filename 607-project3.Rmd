---
title: "DATA 607 Project 3"
author: "Evelyn Bartley, Dhanya Nair, Alexander Simon, AJ Strauman-Scott, Peter Thompson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load database interface package
if (!require(RPostgres)) print("RPostgres package not installed")
library(RPostgres)


# Establish database connection
sql_db <- dbConnect(
  RPostgres::Postgres(),
  dbname = "project3",
  host = '35.199.10.112',
  port = 5432,
  user = 'group',
  password = '1234'
)
```

## Introduction

```{r load-libraries, warning=FALSE, message=FALSE}
library(tidyverse)
```

## Data overview:

This report is based on a survey of 89,184 software developers from 185 countries around the world. This is the number of responses we consider “qualified” for analytical purposes based on consenting to share their information in this survey and finishing all the required questions; approximately 2,000 responses were not included in this analysis. The survey was fielded from May 8, 2023 to May 19, 2023.

```{r load-data}
#Load the original .csv file from Stack Overflow
survey_2023 <- read.csv("https://media.githubusercontent.com/media/alexandersimon1/Data607/main/Project3/survey_results_public.csv", header = TRUE)
```

We filter the survey for only the observations of interest - those who identified as "Data scientists or Machine-Learning Specialist" and "a developer by profession", so that all survey results reflect our population of interest - professional, working data scientists.

```{r filter-data}
# Filter for the observations of interest
datascientists_2023 <- survey_2023 %>% 
  filter(DevType == "Data scientist or machine learning specialist",
         MainBranch == "I am a developer by profession")
```

### Database Design

Describe philosophy behind normalization

Generated ID tables in PostgreSQL for all the possible answers from the survey for each topic we want to examine.

## Data Tidying

The survey data will be tidied in R, pushed to pre-made relational tables in PostgreSQL, and then analyzed through R.

### Tidy the Data

**NOTE to authors** - because there is already a table with the USD value of an observation's compensation already converted, I'm using that column for our 'compensation' column

```{r tidy-ds_2023, warning=FALSE}
# Match df to 'employee' table format in DB
employees_df <- datascientists_2023 %>% 
  mutate(response_id = ResponseId,
         country = as.factor(Country),
         job_title = as.factor(DevType),
         industry = as.factor(Industry), 
         education = as.factor(EdLevel),
         office_location = as.factor(RemoteWork),
         compensation = as.numeric(ConvertedCompYearly),
         years = as.numeric(YearsCodePro),
        language = LanguageHaveWorkedWith,
        platform = PlatformHaveWorkedWith,
        database = DatabaseHaveWorkedWith,
        collab_tools = NEWCollabToolsHaveWorkedWith,
        ai_tools = AIDevHaveWorkedWith
  ) %>% 
  select(response_id, 
         country,
         industry,
         education,
         office_location,
         compensation,
         years,
         language,
         platform,
         database,
         collab_tools,
         ai_tools)
```

### Normalize Multiple Answers

Our variables of interest were all in response to select-all-that-apply survey questions. In order to record each of the answers, each of the 5 multi-choice answer columns must be split so that there is one answer per column in a relational table in the database.

We pivot our survey results longer so that every checkbox has an observation. But if we keep them in the same table, we'll double our data.

We convert multiple choice questions to factors for ease-of-use later.

```{r create-factors}
# Columns of interest to factors
employees_df <- employees_df %>% 
  mutate(language_id = as.factor(language),
         platform_id = as.factor(platform),
         db_id = as.factor(database),
         tool_id = as.factor(ifelse(collab_tools == "Unknown", NA, collab_tools)),
         ai_id = as.factor(ifelse(ai_tools == "Unknown", NA, ai_tools))
  )
```

We define functions for algorithms we will run repeatedly. I tried to write a function that did all the steps to normalize each survey question, but it was too nuanced.

```{r define-functions}
# Define split_and_unnest function
split_and_unnest <- function(data, columns, na_replace = "") {
  split_data <- lapply(columns, function(col) str_split(data[[col]], ";"))
  names(split_data) <- columns
  
  split_data <- lapply(split_data, function(x) replace(x, is.na(x), na_replace))
  
  data <- data %>%
    mutate(across(all_of(columns), ~split_data[[cur_column()]])) %>%
    unnest(cols = all_of(columns))
  
  return(data)
}

# define convert_to_factors function
convert_to_factors <- function(data, columns) {
  data <- data %>%
    mutate(
      across(all_of(columns), as.factor)
    )
  return(data)
}
```

Write the employees table to the database.

```{r write-employees-table}
# Create employees table
employees_sql <- employees_df %>% 
  select(response_id, country, industry, education, office_location, compensation, years)

# Write to DB
# Line removed to prevent data duplication
```

Normalize each survey question we are interested in to its own table.

```{r write-employee-languages}
# Create df to write to 'employee-languages' table
languages_df <- employees_df %>%
  select(response_id, language_id) %>% 
  split_and_unnest('language_id', na_replace = "Unknown") 
  
# Query id table
language_id <- dbGetQuery(sql_db, "SELECT * FROM id_languages") %>%
  convert_to_factors(c("language_id", "language"))

# Swap values for id numbers
languages_df$language_id <- language_id$language_id[match(languages_df$language_id, language_id$language)]  %>% as.numeric()

# Write to Db table
# Line removed to prevent data duplication
```

Automated as much as I could in the function definitions above. Repeat these steps for the other tables

```{r write-employee-platforms}
# Create df to write to 'employee-platforms' table
platforms_df <- employees_df %>%
  select(response_id, platform_id) %>% 
  split_and_unnest('platform_id', na_replace = "Unknown") 

# Query id table
platform_id <- dbGetQuery(sql_db, "SELECT * FROM id_platforms") %>%
  convert_to_factors(c("platform_id", "platform"))

# Swap values for id numbers
platforms_df$platform_id <- platform_id$platform_id[match(platforms_df$platform_id, platform_id$platform)]  %>% as.numeric()

# Write to Db table
# Line removed to prevent data duplication
```

```{r write-employee-databases}
# Create df to write to 'employee-databases' table
db_df <- employees_df %>%
  select(response_id, db_id) %>% 
  split_and_unnest('db_id', na_replace = "Unknown") 

# Query id table
db_id <- dbGetQuery(sql_db, "SELECT * FROM id_databases") %>%
  convert_to_factors(c("db_id", "database"))

# Swap values for id numbers
db_df$db_id <- db_id$db_id[match(db_df$db_id, db_id$database)]  %>% as.numeric()

# Write to DB table
# Line removed to prevent data duplication
```

```{r write-employee-collab-tools}
# Create df to write to 'employee_collab_tools' table
tools_df <- employees_df %>%
  select(response_id, tool_id) %>% 
  split_and_unnest('tool_id', na_replace = "Unknown") 

# Query id table
tool_id <- dbGetQuery(sql_db, "SELECT * FROM id_collab_tools") %>%
  convert_to_factors(c("tool_id", "tool"))

# Swap values for id numbers
tools_df$tool_id <- tool_id$tool_id[match(tools_df$tool_id, tool_id$tool)] %>% as.numeric()

# Write to DB table
# Line removed to prevent data duplication
```

```{r write-employee-ai-tools}
# Create df to write to 'employee_ai_tools' table
ai_df <- employees_df %>%
  select(response_id, ai_id) %>% 
  split_and_unnest('ai_id', na_replace = "Unknown") 

# Query id table
ai_id <- dbGetQuery(sql_db, "SELECT * FROM id_ai_tools") %>%
  convert_to_factors(c("ai_id", "ai_tool"))

# Swap values for id numbers
ai_df$ai_id <- ai_id$ai_id[match(ai_df$ai_id, ai_id$ai_id)] %>% as.numeric()

# Write to DB table
# Line removed to prevent data duplication
```

NOTE TO AUTHORS: I've removed the dbWriteTable commands that I used because it causes errors for duplicate data if left in. All write lines were variations on: `dbWriteTable(sql_db, "employees_ai_tools", ai_df, append = TRUE)`

All our relational tables are created and have their data.

Our SQL relational database is now clean and tidy! I have removed the functions that write to the SQL database so our tables don't get overwritten. They were all variations of `dbWriteTable(sql_db, "employees_language", languages_df, append = TRUE)`

This is diagram of our relational database:

![](images/ER_diagram.png){width="1091"}

## Analysis

Do analysis here!

```{r list-tables}
dbListTables(sql_db)
library(dplyr)
library(tidyverse)
library(forcats)

# count the occurrence of languages in the survey , group the occurrences more than less than 50 times into "others"  and finally arrange in the descending order and plot in a column chart.
languages_df %>% 
  count(language_id)%>% 
  mutate(language_id = if_else(n > 100, language_id, "others")) %>% 
  ggplot(aes(x = fct_reorder(language_id,n), y = n)) + 
    geom_col(fill = "lightblue") + 
  coord_flip() +
    labs(x = "languages", y = "most valuable data science - language skills", caption = "https://insights.stackoverflow.com/survey/")


```
 

```

## Conclusion

### Bibliography
